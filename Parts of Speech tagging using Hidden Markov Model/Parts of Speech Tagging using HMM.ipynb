{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This notebook implements parts of speech (POS) tagging using Hidden Markov Model (HMM). Implementation of Viterbi Algorithm is also shown later."},{"metadata":{},"cell_type":"markdown","source":"## Say words = w1....wN and tags = t1..tN ,then,\n## P(tags | words) is_proportional_to  product P(ti| t{i-1}) * P(wi | ti)\n## To find the best tag sequence for a given sequence of words,we want to find the tag sequence that has the maximum P(tags | words)\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import brown","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brown.tagged_sents()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Now, we will estimate P(wi|ti) from corpus data by calculating Maximum Likelihood Estimation MLE: P(wi | ti)=count(wi,ti)/count(ti)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of all the unique tags from the corpus\n\nbrown_word_tags=[]\n\n#Manually adding the start and the end tag\nfor brown_sent in brown.tagged_sents():\n    brown_word_tags.append(('START','START'))\n    \n    for words,tag in brown_sent:\n        brown_word_tags.extend([(tag[:2],words)])\n        \n    brown_word_tags.append(('END','END'))\n    ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the continuous frequency distribution for the words which are tagged\ncfd_tag_words=nltk.ConditionalFreqDist(brown_word_tags)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the conditional probability distribution\ncpd_tag_words=nltk.ConditionalProbDist(cfd_tag_words,nltk.MLEProbDist)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The probability of an adjective (JJ) being 'smart' is\", cpd_tag_words[\"JJ\"].prob(\"smart\"))\nprint(\"The probability of a verb (VB) being 'try' is\", cpd_tag_words[\"VB\"].prob(\"try\"))","execution_count":26,"outputs":[{"output_type":"stream","text":"The probability of an adjective (JJ) being 'smart' is 0.00027780092785509904\nThe probability of a verb (VB) being 'try' is 0.0010790559555256297\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n# P(ti | t{i-1}) = count(t{i-1}, ti) / count(t{i-1})"},{"metadata":{"trusted":true},"cell_type":"code","source":"brown_tags=[]\nfor tag, words in brown_word_tags:\n    brown_tags.append(tag)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#make conditional frequency distribution: count(t{i-1} ti)\ncfd_tags=nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make conditional probability distribution, using maximum likelihood estimate: P(ti | t{i-1})\ncpd_tags=nltk.ConditionalProbDist(cfd_tags,nltk.MLEProbDist)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The probability of DT occuring after NN is : ', cpd_tags[\"NN\"].prob(\"DT\"))\nprint('The probability of VB occuring after NN is : ', cpd_tags[\"NN\"].prob(\"VB\"))","execution_count":25,"outputs":[{"output_type":"stream","text":"The probability of DT occuring after NN is :  0.0018349509874933604\nThe probability of VB occuring after NN is :  0.0646359290427087\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"\n## The probability of the tag sequence \"PP VB NN\" for the word sequence \"I love food\"?\n## P(START) * P(PP|START) * P(I | PP) * P(VB | PP) * P(love | VB) * P(TO | VB) * P(food | NN) * P(END | VB)"},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tag_words[\"PP\"].prob(\"I\") * \\\n                   cpd_tags[\"PP\"].prob(\"VB\") * cpd_tag_words[\"VB\"].prob(\"love\") * \\\n                   cpd_tags[\"VB\"].prob(\"NN\") * cpd_tag_words[\"PP\"].prob(\"food\") * \\\n                   cpd_tags[\"NN\"].prob(\"END\")","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The probability of sentence 'I love food' having the tag sequence 'START PP VB PP END' is : \", prob_tagsequence)","execution_count":29,"outputs":[{"output_type":"stream","text":"The probability of sentence 'I love food' having the tag sequence 'START PP VB PP END' is :  0.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viterbi:\n# If we have a word sequence, what is the best tag sequence?\n#\n# The method above lets us determine the probability for a single tag sequence.\n# But in order to find the best tag sequence, we need the probability\n# for _all_ tag sequence.\n# What Viterbi gives us is just a good way of computing all those many probabilities\n# as fast as possible.\n\n\ndistinct_brown_tags=set(brown_tags)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sentences=[\"I\",\"love\",\"spicy\",\"food\"]\nlen_sample_sentence=len(sample_sentences)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each step i in 1 .. sentlen,\n# store a dictionary\n# that maps each tag X\n# to the probability of the best tag sequence of length i that ends in X\n\n\nviterbi_tags={}\nviterbi_backpointer={}\n\nfor tag in distinct_brown_tags:\n    if tag==\"START\":\n        continue\n    viterbi_tags[tag]=cpd_tags[\"START\"].prob(tag)*cpd_tag_words[tag].prob(sample_sentences[0])\n    viterbi_backpointer[tag]=\"START\"","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each step i in 1 .. sentlen,\n# store a dictionary\n# that maps each tag X\n# to the probability of the best tag sequence of length i that ends in X\n\n\n\nviterbi_main=[]\nbackpointer_main=[]\n\nviterbi_main.append(viterbi_tags)\nbackpointer_main.append(viterbi_backpointer)\n\ncurrent_best=max(viterbi_tags.keys(),key=lambda tag: viterbi_tags[tag])\n\n","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Word\", \"'\" + sample_sentences[0] + \"'\", \"current best two-tag sequence:\", viterbi_backpointer[current_best], current_best)\n","execution_count":39,"outputs":[{"output_type":"stream","text":"Word 'I' current best two-tag sequence: START PP\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfor index in range(1,len_sample_sentence):\n    curr_viterbi={}\n    curr_backpointer={}\n    prev_viterbi=viterbi_main[-1]\n    \n    for brown_tag in distinct_brown_tags:\n        \n        if brown_tag != \"START\":\n            # if this tag is X and the current word is w, then\n            # find the previous tag Y such that\n            # the best tag sequence that ends in X\n            # actually ends in Y X\n            # that is, the Y that maximizes\n            # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n            # The following command has the same notation\n            # that you saw in the sorted() command.\n            prev_best = max(prev_viterbi.keys(),\n                                key=lambda prevtag: \\\n                                    prev_viterbi[prevtag] * cpd_tags[prevtag].prob(brown_tag) * cpd_tag_words[brown_tag].prob(\n                                        sample_sentences[index]))\n\n            curr_viterbi[brown_tag] = prev_viterbi[prev_best] * \\\n                                cpd_tags[prev_best].prob(brown_tag) * cpd_tag_words[brown_tag].prob(sample_sentences[index])\n            curr_backpointer[brown_tag] = prev_best\n\n    current_best = max(curr_viterbi.keys(), key=lambda tag: curr_viterbi[tag])\n    print(\"Word\", \"'\" + sample_sentences[index] + \"'\", \"current best two-tag sequence:\", curr_backpointer[current_best], current_best)\n\n\n    viterbi_main.append(curr_viterbi)\n    backpointer_main.append(curr_backpointer)\n","execution_count":44,"outputs":[{"output_type":"stream","text":"Word 'love' current best two-tag sequence: PP NN\nWord 'spicy' current best two-tag sequence: VB JJ\nWord 'food' current best two-tag sequence: JJ NN\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now find the probability of each tag\n# to have \"END\" as the next tag,\n# and use that to find the overall best sequence\n\n\n\nprev_viterbi = viterbi_main[-1]\nprev_best = max(prev_viterbi.keys(),\n                    key=lambda prev_tag: prev_viterbi[prev_tag] * cpd_tags[prev_tag].prob(\"END\"))\n\nprob_tag_sequence = prev_viterbi[prev_best] * cpd_tags[prev_best].prob(\"END\")\n\n\nbest_tag_sequence = [\"END\", prev_best]\n# invert the list of backpointers\nbackpointer_main.reverse()\n\n# go backwards through the list of backpointers\n# (or in this case forward, because we have inverter the backpointer list)\n# in each case:\n# the following best tag is the one listed under\n# the backpointer for the current best tag\ncurrent_best_tag = prev_best\nfor backpointer in backpointer_main:\n    best_tag_sequence.append(backpointer[current_best_tag])\n    current_best_tag = backpointer[current_best_tag]\n","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_tag_sequence.reverse()","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The sentence given is :\")\nfor word in sample_sentences:\n    print (word,\"\",)","execution_count":49,"outputs":[{"output_type":"stream","text":"The sentence given is :\nI \nlove \nspicy \nfood \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The best tag sequence using HMM for the given sentence is : \")\n\n\nfor best_tag in best_tag_sequence:\n    print (best_tag, \"\",)","execution_count":50,"outputs":[{"output_type":"stream","text":"The best tag sequence using HMM for the given sentence is : \nSTART \nPP \nVB \nJJ \nNN \nEND \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The probability of the best tag sequence printed above is given by : \", prob_tag_sequence)","execution_count":51,"outputs":[{"output_type":"stream","text":"The probability of the best tag sequence printed above is given by :  1.085507090627119e-18\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}